{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Trading Agent - Getting Started\n",
    "\n",
    "This notebook demonstrates how to use the RL trading agent framework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data.data_loader import DataLoader\n",
    "from environment.trading_env import TradingEnvironment\n",
    "from agents.dqn_agent import DQNAgent\n",
    "from visualization.plotting import TradingVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "data_loader = DataLoader('../config/config.yaml')\n",
    "train_data, test_data = data_loader.prepare_data('AAPL')\n",
    "\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")\n",
    "print(f\"\\nFeatures: {list(train_data.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trading environment\n",
    "env = TradingEnvironment(train_data, '../config/config.yaml')\n",
    "\n",
    "print(f\"Observation space: {env.observation_space.shape}\")\n",
    "print(f\"Action space: {env.action_space.n}\")\n",
    "print(f\"Initial balance: ${env.initial_balance:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Agent Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DQN agent\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = DQNAgent(state_size, action_size, '../config/config.yaml')\n",
    "\n",
    "print(f\"Agent initialized with state size: {state_size}\")\n",
    "print(f\"Action size: {action_size}\")\n",
    "print(f\"Device: {agent.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quick Training Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick training demo (10 episodes)\n",
    "episodes = 10\n",
    "rewards = []\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    while True:\n",
    "        action = agent.act(state, training=True)\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        \n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        \n",
    "        if len(agent.memory) > agent.batch_size:\n",
    "            agent.replay()\n",
    "        \n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done or truncated:\n",
    "            break\n",
    "    \n",
    "    rewards.append(total_reward)\n",
    "    print(f\"Episode {episode + 1}: Reward = {total_reward:.2f}, Portfolio = ${info['portfolio_value']:.0f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(rewards)\n",
    "plt.title('Training Rewards (Demo)')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the agent\n",
    "test_env = TradingEnvironment(test_data, '../config/config.yaml')\n",
    "state, _ = test_env.reset()\n",
    "portfolio_values = [test_env.initial_balance]\n",
    "\n",
    "while True:\n",
    "    action = agent.act(state, training=False)  # No exploration\n",
    "    next_state, reward, done, truncated, info = test_env.step(action)\n",
    "    \n",
    "    portfolio_values.append(info['portfolio_value'])\n",
    "    state = next_state\n",
    "    \n",
    "    if done or truncated:\n",
    "        break\n",
    "\n",
    "# Plot results\n",
    "visualizer = TradingVisualizer()\n",
    "visualizer.plot_portfolio_performance(portfolio_values, test_env.trade_history)\n",
    "\n",
    "# Print final stats\n",
    "stats = test_env.get_portfolio_stats()\n",
    "print(f\"\\nFinal Portfolio Value: ${stats['final_portfolio_value']:,.2f}\")\n",
    "print(f\"Total Return: {stats['total_return']:.2f}%\")\n",
    "print(f\"Total Trades: {stats['total_trades']}\")\n",
    "print(f\"Win Rate: {stats['win_rate']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Next Steps\n",
    "\n",
    "- Run the full training script: `python ../train_agent.py`\n",
    "- Evaluate trained models: `python ../evaluate_agent.py`\n",
    "- Experiment with different hyperparameters in `config/config.yaml`\n",
    "- Try different stocks by modifying the symbols list\n",
    "- Implement additional technical indicators"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
